{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOME GOOD CODING PRACTICES FOR DATA SCIENTISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Automate repetitive tasks through functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a common task in Machine Learning projects pipeline like tuning hyperparameters model. Suppose you’re working in an image classification project and you would like to try Support Vector Machine classifier (SVC) and Artificial Neural Network (ANN) with Multi-layer Perceptron classifier (MLPClassifier()). To tune the hyperparameters model we’ll be using the GridSearchCV() method for each one from scikit-learn Machine Learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc model\n",
    "ml_model = SVC()\n",
    "hyper_parameter_candidates = {\"C\": [1e-4, 1e-2, 1, 1e2, 1e4],\n",
    "    \"gamma\": [1e-3, 1e-2, 1, 1e2, 1e3],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = GridSearchCV(estimator=ml_model, \n",
    "    param_grid=hyper_parameter_candidates,   \n",
    "    scoring=scoring_parameter, cv=cv_fold)\n",
    "classifier_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ann model\n",
    "ml_model = MLPClassifier()\n",
    "hyper_parameter_candidates = {\"hidden_layer_sizes\":[(20), (50), \n",
    "   (100)], \"max_iter\":[500, 800, 1000], \n",
    "   \"activation\":[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "   \"solver\":[\"lbfgs\", \"sgd\", \"adam\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = GridSearchCV(estimator=ml_model,    \n",
    "   param_grid=hyper_parameter_candidates,  \n",
    "   scoring=scoring_parameter, cv=cv_fold)\n",
    "classifier_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code above, I could write a simple function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    classifier_model = GridSearchCV(estimator=ml_model, \n",
    "       param_grid=hyper_parameter_candidates, \n",
    "       scoring=scoring_parameter, cv=cv_fold)       \n",
    "    classifier_model.fit(X_train, y_train)  \n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Implement error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add some error handling code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    try:   \n",
    "        classifier_model = GridSearchCV(estimator=ml_model, \n",
    "           param_grid=hyper_parameter_candidates, \n",
    "           scoring=scoring_parameter, cv=cv_fold)       \n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        exception_message = sys.exc_info()[0]\n",
    "        print(\"An error occurred. {}\".format(exception_message))\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some planning, I should be able to write a better generic function print_exception_message() to print my exception messages and use it for all my functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_exception_message(message_orientation=\"horizontal\"):\n",
    "    \"\"\"\n",
    "    print full exception message\n",
    "   :param message_orientation: horizontal or vertical\n",
    "   :return None   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        exc_type, exc_value, exc_tb = sys.exc_info()           \n",
    "        file_name, line_number, procedure_name, line_code =  traceback.extract_tb(exc_tb)[-1]      \n",
    "        time_stamp = \" [Time Stamp]: \" + str(time.strftime(\" %Y-%m-%d %I:%M:%S %p\"))\n",
    "        file_name = \" [File Name]: \" + str(file_name)\n",
    "        procedure_name = \" [Procedure Name]: \" +  str(procedure_name)\n",
    "        error_message = \" [Error Message]: \" + str(exc_value)       \n",
    "        error_type = \" [Error Type]: \" + str(exc_type)                   \n",
    "        line_number = \" [Line Number]: \" + str(line_number)               \n",
    "        line_code = \" [Line Code]: \" + str(line_code)\n",
    "        if (message_orientation == \"horizontal\"):\n",
    "            print( \"An error occurred:{};{};{};{};{};{}; {}\".format(time_stamp, file_name, procedure_name, \n",
    "               error_message, error_type, line_number, line_code))\n",
    "        elif (message_orientation == \"vertical\"):\n",
    "            print( \"An error occurred:\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\".format(time_stamp, file_name, \n",
    "               procedure_name, error_message, error_type,        \n",
    "               line_number, line_code))\n",
    "        else:\n",
    "            pass                   \n",
    "    except:\n",
    "        exception_message = sys.exc_info()[0]\n",
    "        print(\"An error occurred. {}\".format(exception_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we implement this function in our code, we’ll use a simple line of code in the exception block only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    try:   \n",
    "        classifier_model = GridSearchCV(estimator=ml_model, \n",
    "           param_grid=hyper_parameter_candidates, \n",
    "           scoring=scoring_parameter, cv=cv_fold)       \n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, for the MLPClassifier() model, we’ll have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = MLPClassifier()\n",
    "hyper_parameter_candidates = {\"hidden_layer_sizes\":[(20), (50), \n",
    "   (100)], \"max_iter\":[500, 800, 1000], \n",
    "   \"activation\":[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "   \"solver\":[\"lbfgs\", \"sgd\", \"adam\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = tune_hyperparameter_model(ml_model, X_train,  \n",
    "   y_train, hyper_parameter_candidates, scoring_parameter, \n",
    "   cv_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll continue updating the function tune_hyperparameter_model() code in the next topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Do not hardcode of default numerical and string parameters including Machine Learning hyperparameters model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to fix these issues is to put all the default numerical and string values in a simple configuration (config.py) file. For security purposes any config could be encrypted as necessary. Let me update our function tune_hyperparameter_model() to handle GridSearchCV() and RandomizedSearchCV() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold, search_cv_type=\"grid\"):   \n",
    "    try:\n",
    "        if (search_cv_type==\"grid\"):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model, \n",
    "               param_grid=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        elif (search_cv_type==\"randomized\"):\n",
    "            classifier_model =  RandomizedSearchCV(estimator=ml_model, \n",
    "               param_distributions=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the input parameter search_cv_type is optional and equal to “grid”. If we create a config.py file with the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SEARCH_CV=\"grid\"\n",
    "RANDOMIZED_SEARCH_CV=\"randomized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our function (import config statement is required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter , cv_fold, search_cv_type=\"grid\"): \n",
    "    try:\n",
    "        if (search_cv_type==config.GRID_SEARCH_CV):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model,   param_grid=hyper_parameter_candidates, \n",
    "            scoring=scoring_parameter, cv=cv_fold) \n",
    "\n",
    "        elif (search_cv_type== config.RANDOMIZED_SEARCH_CV):\n",
    "            classifier_model = RandomizedSearchCV(estimator=ml_model, param_distributions=hyper_parameter_candidates, \n",
    "            scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice generic function that can be used everywhere and we can change the logic of it by changing the config.py file only. Nothing has been hardcoded here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Provide code comments, especially Dostring comments for modules, functions, classes, or methods definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python programming language has a specific standard way of writing comments for class objects and function procedures. It is called Docstring — "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“docstring is a literal string that occurs as the first statement in a module, function, class, or method definition. Such a docstring becomes the __doc__ special attribute of that object”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the Docstring for our function tune_hyperparameter_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter , cv_fold, \n",
    "search_cv_type=\"grid\"):   \n",
    "    \"\"\"\n",
    "    apply grid search cv and randomized search cv algorithms to \n",
    "    find optimal hyperparameters model \n",
    "    :param ml_model: defined machine learning model\n",
    "    :param X_train: feature training data\n",
    "    :param y_train: target (label) training data\n",
    "    :param hyper_parameter_candidates: dictionary of \n",
    "     hyperparameter candidates\n",
    "    :param scoring_parameter: parameter that controls what metric \n",
    "     to apply to the evaluated model\n",
    "    :param cv_fold: number of cv divided folds\n",
    "    :param search_cv_type: type of search cv (gridsearchcv or \n",
    "     randomizedsearchcv)\n",
    "    :return classifier_model: defined classifier model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (search_cv_type==config.GRID_SEARCH_CV):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model, \n",
    "               param_grid=hyper_parameter_candidates, \n",
    "               scoring=scoring_paramete, cv=cv_fold)\n",
    "        elif (search_cv_type==config.RANDOMIZED_SEARCH_CV):\n",
    "            classifier_model = RandomizedSearchCV(estimator=ml_model, \n",
    "               param_distributions=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have a real production Python function implemented now. We should be able to include this function to a base (super) class to be reused in any Machine Learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Ensure that programs unit tests implemented "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Every computer program created needs to have unit tests. We need to implement unit tests in Machine Learning projects as well. One of the most important unit tests in Machine Learning classification projects is the calculation of the __Accuracy Classification Score__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I want my test data to have a high Accuracy Score compared with a required Threshold Accuracy Score value. If the calculated Accuracy Score is greater than or equal to Threshold Accuracy Score value, then I’ll use the results to make the required business classification decisions. If not, I may need to do the following: retrain my previous model, try other classification models, collect more data if possible, speak to a domain expert to get more context information, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a simple unit test code shown below. This test for done using the Fashion-MNIST image datasets and a trained ANN model file fashion_mnist_ann_classification.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import config\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from fashion_mnist_ann import calculate_accuracy_score\n",
    "\n",
    "class ANNTest(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    ann unit test class\n",
    "    \"\"\"    \n",
    "    def testAccuracyScore(self):\n",
    "        \"\"\"\n",
    "        accuracy classification score unit test\n",
    "        \"\"\"\n",
    "        #        get data folder path\n",
    "        data_folder_path = config.DATA_FOLDER_PATH\n",
    "        #         define fashion mnist test pandas dataframe\n",
    "        fashion_mnist_test = config.FASHION_MNIST_TEST\n",
    "        df_fashion_mnist_test = pd.read_csv(os.path.join(data_folder_path,  fashion_mnist_test), header=None)\n",
    "        #         get number of columns\n",
    "        df_fashion_mnist_test_columns = df_fashion_mnist_test.shape[1]\n",
    "        #         select y test label\n",
    "        target_column_number = config.TARGET_COLUMN_NUMBER\n",
    "        y_test = df_fashion_mnist_test.iloc[:,0:target_column_number]\n",
    "        #         flat y test label\n",
    "        y_test_flattened = y_test.values.ravel()\n",
    "        #         select X test features\n",
    "        X_test =       df_fashion_mnist_test.iloc[:,target_column_number:df_fashion_mnist_test_columns]\n",
    "        #         normalize X test features with min-max scaling\n",
    "        X_test = (X_test.astype(\"float32\") - config.XMIN) /  (config.XMAX - config.XMIN)\n",
    "        #         open and close fashion mnist model pkl file\n",
    "        mlp_classifier_model_pkl = open(config.FASHION_MNIST_MODEL_FILE, \"rb\")      \n",
    "        mlp_classifier_model_file = pickle.load(mlp_classifier_model_pkl)\n",
    "        mlp_classifier_model_pkl.close()#         get y predict test\n",
    "        y_predict_test = mlp_classifier_model_file.predict(X_test)\n",
    "        #         calculate accuracy classification score             \n",
    "        accuracy_score_value = calculate_accuracy_score(y_test_flattened,  y_predict_test)\n",
    "        #         test for accuracy score value greater than or equal to  threshold accuracy score\n",
    "        self.assertGreaterEqual(accuracy_score_value, \n",
    "           config.THRESHOLD_ACCURACY_SCORE, \"Test Accuracy Score Failed.\")              \n",
    "        \n",
    "#if __name__ == \"__main__\":   \n",
    " #   unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see I have commented every line of code for you to understand very clearly how this unit test runs. In real production projects, this code will be encapsulated in a main derived class file to run train, validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __calculate_accuracy_score()__ calculates the __Accuracy Classification Score__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_score(label_true, label_predict):       \n",
    "    \"\"\"\n",
    "    calculate accuracy classification score\n",
    "    :param label_true: label true values\n",
    "    :param label_predict: label predicted values\n",
    "    return: accuracy classification score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        accuracy_score_value = accuracy_score(label_true, \n",
    "           label_predict) * 100\n",
    "        accuracy_score_value = float(\"{0:0.2f}\".format(accuracy_score_value)) \n",
    "    except:    \n",
    "        print_exception_message()         \n",
    "    return accuracy_score_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run some testing. From the config.py file the Threshold Accuracy Score is equal 86%. After we run the test, it would pass if the calculated Accuracy Score were greater than or equal to 86%. If Threshold Accuracy Score increases to 90%, the test would fail with the following message:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AssertionError: 86.33 not greater than or equal to 90 : Test Accuracy Score Failed.\n",
    "Ran 1 test in 1.415s\n",
    "FAILED (failures=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It’s up to the Data Analytics team to decide, if this score is sufficient enough to be used for real business data classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__“Be a good Python Software Engineer, not a good “Pythonic” Software Engineer”__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE: https://medium.com/@ernest.bonat/refactoring-python-code-for-machine-learning-projects-python-spaghetti-code-everywhere-daaa6c116bd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
