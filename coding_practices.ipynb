{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOME GOOD CODING PRACTICES FOR DATA SCIENTISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Automate repetitive tasks through functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a common task in Machine Learning projects pipeline like tuning hyperparameters model. Suppose you’re working in an image classification project and you would like to try Support Vector Machine classifier (SVC) and Artificial Neural Network (ANN) with Multi-layer Perceptron classifier (MLPClassifier()). To tune the hyperparameters model we’ll be using the GridSearchCV() method for each one from scikit-learn Machine Learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc model\n",
    "ml_model = SVC()\n",
    "hyper_parameter_candidates = {\"C\": [1e-4, 1e-2, 1, 1e2, 1e4],\n",
    "    \"gamma\": [1e-3, 1e-2, 1, 1e2, 1e3],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "    \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = GridSearchCV(estimator=ml_model, \n",
    "    param_grid=hyper_parameter_candidates,   \n",
    "    scoring=scoring_parameter, cv=cv_fold)\n",
    "classifier_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ann model\n",
    "ml_model = MLPClassifier()\n",
    "hyper_parameter_candidates = {\"hidden_layer_sizes\":[(20), (50), \n",
    "   (100)], \"max_iter\":[500, 800, 1000], \n",
    "   \"activation\":[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "   \"solver\":[\"lbfgs\", \"sgd\", \"adam\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = GridSearchCV(estimator=ml_model,    \n",
    "   param_grid=hyper_parameter_candidates,  \n",
    "   scoring=scoring_parameter, cv=cv_fold)\n",
    "classifier_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code above, I could write a simple function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    classifier_model = GridSearchCV(estimator=ml_model, \n",
    "       param_grid=hyper_parameter_candidates, \n",
    "       scoring=scoring_parameter, cv=cv_fold)       \n",
    "    classifier_model.fit(X_train, y_train)  \n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Implement error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add some error handling code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    try:   \n",
    "        classifier_model = GridSearchCV(estimator=ml_model, \n",
    "           param_grid=hyper_parameter_candidates, \n",
    "           scoring=scoring_parameter, cv=cv_fold)       \n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        exception_message = sys.exc_info()[0]\n",
    "        print(\"An error occurred. {}\".format(exception_message))\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some planning, I should be able to write a better generic function print_exception_message() to print my exception messages and use it for all my functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_exception_message(message_orientation=\"horizontal\"):\n",
    "    \"\"\"\n",
    "    print full exception message\n",
    "   :param message_orientation: horizontal or vertical\n",
    "   :return None   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        exc_type, exc_value, exc_tb = sys.exc_info()           \n",
    "        file_name, line_number, procedure_name, line_code =  traceback.extract_tb(exc_tb)[-1]      \n",
    "        time_stamp = \" [Time Stamp]: \" + str(time.strftime(\" %Y-%m-%d %I:%M:%S %p\"))\n",
    "        file_name = \" [File Name]: \" + str(file_name)\n",
    "        procedure_name = \" [Procedure Name]: \" +  str(procedure_name)\n",
    "        error_message = \" [Error Message]: \" + str(exc_value)       \n",
    "        error_type = \" [Error Type]: \" + str(exc_type)                   \n",
    "        line_number = \" [Line Number]: \" + str(line_number)               \n",
    "        line_code = \" [Line Code]: \" + str(line_code)\n",
    "        if (message_orientation == \"horizontal\"):\n",
    "            print( \"An error occurred:{};{};{};{};{};{}; {}\".format(time_stamp, file_name, procedure_name, \n",
    "               error_message, error_type, line_number, line_code))\n",
    "        elif (message_orientation == \"vertical\"):\n",
    "            print( \"An error occurred:\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\\n{}\".format(time_stamp, file_name, \n",
    "               procedure_name, error_message, error_type,        \n",
    "               line_number, line_code))\n",
    "        else:\n",
    "            pass                   \n",
    "    except:\n",
    "        exception_message = sys.exc_info()[0]\n",
    "        print(\"An error occurred. {}\".format(exception_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we implement this function in our code, we’ll use a simple line of code in the exception block only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold):   \n",
    "    try:   \n",
    "        classifier_model = GridSearchCV(estimator=ml_model, \n",
    "           param_grid=hyper_parameter_candidates, \n",
    "           scoring=scoring_parameter, cv=cv_fold)       \n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, for the MLPClassifier() model, we’ll have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = MLPClassifier()\n",
    "hyper_parameter_candidates = {\"hidden_layer_sizes\":[(20), (50), \n",
    "   (100)], \"max_iter\":[500, 800, 1000], \n",
    "   \"activation\":[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "   \"solver\":[\"lbfgs\", \"sgd\", \"adam\"]}\n",
    "scoring_parameter = \"accuracy\"\n",
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "classifier_model = tune_hyperparameter_model(ml_model, X_train,  \n",
    "   y_train, hyper_parameter_candidates, scoring_parameter, \n",
    "   cv_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll continue updating the function tune_hyperparameter_model() code in the next topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Do not hardcode of default numerical and string parameters including Machine Learning hyperparameters model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to fix these issues is to put all the default numerical and string values in a simple configuration (config.py) file. For security purposes any config could be encrypted as necessary. Let me update our function tune_hyperparameter_model() to handle GridSearchCV() and RandomizedSearchCV() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter, cv_fold, search_cv_type=\"grid\"):   \n",
    "    try:\n",
    "        if (search_cv_type==\"grid\"):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model, \n",
    "               param_grid=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        elif (search_cv_type==\"randomized\"):\n",
    "            classifier_model =  RandomizedSearchCV(estimator=ml_model, \n",
    "               param_distributions=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the input parameter search_cv_type is optional and equal to “grid”. If we create a config.py file with the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SEARCH_CV=\"grid\"\n",
    "RANDOMIZED_SEARCH_CV=\"randomized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our function (import config statement is required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter , cv_fold, search_cv_type=\"grid\"): \n",
    "    try:\n",
    "        if (search_cv_type==config.GRID_SEARCH_CV):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model,   param_grid=hyper_parameter_candidates, \n",
    "            scoring=scoring_parameter, cv=cv_fold) \n",
    "\n",
    "        elif (search_cv_type== config.RANDOMIZED_SEARCH_CV):\n",
    "            classifier_model = RandomizedSearchCV(estimator=ml_model, param_distributions=hyper_parameter_candidates, \n",
    "            scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice generic function that can be used everywhere and we can change the logic of it by changing the config.py file only. Nothing has been hardcoded here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Provide code comments, especially Dostring comments for modules, functions, classes, or methods definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python programming language has a specific standard way of writing comments for class objects and function procedures. It is called Docstring — "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“docstring is a literal string that occurs as the first statement in a module, function, class, or method definition. Such a docstring becomes the __doc__ special attribute of that object”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the Docstring for our function tune_hyperparameter_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "def tune_hyperparameter_model(ml_model, X_train, y_train, hyper_parameter_candidates, scoring_parameter , cv_fold, \n",
    "search_cv_type=\"grid\"):   \n",
    "    \"\"\"\n",
    "    apply grid search cv and randomized search cv algorithms to \n",
    "    find optimal hyperparameters model \n",
    "    :param ml_model: defined machine learning model\n",
    "    :param X_train: feature training data\n",
    "    :param y_train: target (label) training data\n",
    "    :param hyper_parameter_candidates: dictionary of \n",
    "     hyperparameter candidates\n",
    "    :param scoring_parameter: parameter that controls what metric \n",
    "     to apply to the evaluated model\n",
    "    :param cv_fold: number of cv divided folds\n",
    "    :param search_cv_type: type of search cv (gridsearchcv or \n",
    "     randomizedsearchcv)\n",
    "    :return classifier_model: defined classifier model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (search_cv_type==config.GRID_SEARCH_CV):\n",
    "            classifier_model = GridSearchCV(estimator=ml_model, \n",
    "               param_grid=hyper_parameter_candidates, \n",
    "               scoring=scoring_paramete, cv=cv_fold)\n",
    "        elif (search_cv_type==config.RANDOMIZED_SEARCH_CV):\n",
    "            classifier_model = RandomizedSearchCV(estimator=ml_model, \n",
    "               param_distributions=hyper_parameter_candidates, \n",
    "               scoring=scoring_parameter, cv=cv_fold)\n",
    "        classifier_model.fit(X_train, y_train)\n",
    "    except:\n",
    "        print_exception_message()\n",
    "    return classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have a real production Python function implemented now. We should be able to include this function to a base (super) class to be reused in any Machine Learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE: https://medium.com/@ernest.bonat/refactoring-python-code-for-machine-learning-projects-python-spaghetti-code-everywhere-daaa6c116bd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
