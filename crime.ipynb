{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training subset: 0.738293\n",
      "Accuracy on the testing subset: 0.373440\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Handling categorical attributes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('~/Documents/datasets/crime.csv',nrows=100000)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "#df = data.apply(encoder.fit_transform)\n",
    "#data[\"Category\"] = encoder.fit_transform(data[\"Category\"].astype('str'))\n",
    "attributes = [ 'Category',  'DayOfWeek', 'PdDistrict', 'Resolution', 'X', 'Y']\n",
    "df = data[attributes]\n",
    "day = pd.get_dummies(df['DayOfWeek'])\n",
    "district = pd.get_dummies(df['PdDistrict'])\n",
    "resolution = pd.get_dummies(df['Resolution'])\n",
    "crime_cat = df[\"Category\"]\n",
    "crimes = df.drop(['Category','DayOfWeek','PdDistrict','Resolution'], axis=1)\n",
    "dat = pd.concat([crimes,day,district,resolution],axis=1)\n",
    "\n",
    "X_train, X_test, y_train ,y_test = train_test_split(dat, crime_cat, random_state= 0)\n",
    "\n",
    "\n",
    "#df.head()\n",
    "forest = RandomForestClassifier(n_jobs=2,max_depth=1000,n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy on the training subset: {:3f}'.format(forest.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training subset: 0.960467\n",
      "Accuracy on the testing subset: 0.573840\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Load some data and do some cleaning\n",
    "'''\n",
    "crimes = pd.read_csv(\"~/Documents/datasets/crime.csv\",nrows = 100000)\n",
    "\n",
    "#crimes['Mapping'] = np.abs(crimes[\"X\"] + crimes[\"Y\"]) \n",
    "crimes.drop(['IncidntNum','Descript','Location'],1,inplace=True)\n",
    "\n",
    "# custom multicolumn transformer\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self, columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "        \n",
    "    def fit(self, X,y=None):\n",
    "        return self # not relevant\n",
    "    def transform(self, X):\n",
    "        ''' \n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all columns in X\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "             for colname,col in enumerate(output):\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def fit_transform(self, X ,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "### you can also use the above class as shown in the line below ###\n",
    "#MultiColumnLabelEncoder(columns = cat_attributes).fit_transform(olympics)\n",
    "                    \n",
    "def column_types(df):\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    \n",
    "    for y in df.columns:\n",
    "        if (df[y].dtype == object):\n",
    "            cat_cols.append(y)\n",
    "        else:\n",
    "            num_cols.append(y)\n",
    "    \n",
    "    return cat_cols, num_cols\n",
    "\n",
    "# Handling categorical attributes\n",
    "class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class MyLabelBinarizer():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = LabelBinarizer( *args, **kwargs)\n",
    "    def fit(self, x, y = 0):\n",
    "        self.encoder.fit(x)\n",
    "        return self\n",
    "    def transform(self,x, y=0):\n",
    "        return self.encoder.transform(x)\n",
    "\n",
    "cat_attributes ,num_attributes = column_types(crimes)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(num_attributes)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attributes)),\n",
    "    ('label_encoder', MultiColumnLabelEncoder())\n",
    "]) \n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])\n",
    "\n",
    "prepared = full_pipeline.fit_transform(crimes)\n",
    "\n",
    "labels = crimes[\"Category\"]\n",
    "\n",
    "X_train, X_test, y_train ,y_test = train_test_split(prepared, labels, random_state= 0,test_size = 0.25)\n",
    "forest = RandomForestClassifier(n_jobs=-1,max_depth=100,n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "label_rf = forest.predict(X_test)\n",
    "print('Accuracy on the training subset: {:3f}'.format(forest.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7196007339903274"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "'''\n",
    "Load some data and do some cleaning\n",
    "#NOTE: This dataset contains one million rows so it takes a considerable time \n",
    "        to load depending on your computing power and memory resources\n",
    "'''\n",
    "data = pd.read_csv('~/Documents/datasets/crime.csv',nrows=100000)\n",
    "\n",
    "data = data.sample(frac=0.05, random_state=1)\n",
    "#data['PdId'] = data['PdId'] / 10e3\n",
    "to_drop = ['IncidntNum','Category','PdId']\n",
    "X_all = data.drop(to_drop,1)\n",
    "y_all = data[\"Category\"]\n",
    "\n",
    "\n",
    "#Standardising the data\n",
    "num_features = ['X','Y']\n",
    "cat_features = ['Descript', 'DayOfWeek', 'Date', 'Time', 'PdDistrict', 'Resolution',\n",
    "       'Address', 'Location']\n",
    "\n",
    "scaled_data = StandardScaler().fit_transform(X_all[num_features])\n",
    "scaled = pd.DataFrame(scaled_data,columns = num_features)\n",
    "dummies = pd.get_dummies(X_all[cat_features],prefix = [col for col, col_data in X_all[cat_features].iteritems()])\n",
    "\n",
    "\n",
    "\n",
    "pca=PCA(n_components=600)\n",
    "\n",
    "x_pca = pca.fit_transform(dummies)\n",
    "\n",
    "#df = dummies.join(scaled)\n",
    "\n",
    "X_train, X_test, y_train ,y_test = train_test_split(x_pca, y_all, random_state= 0,test_size = 0.25)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresssion...\n",
      "Done in 1.747060775756836 seconds...\n",
      "Accuracy on the training subset: 0.980533\n",
      "Accuracy on the testing subset: 0.922400\n",
      "\n",
      "Support Vector Machine...\n",
      "Done in 15.893916845321655 seconds...\n",
      "Accuracy on the training subset: 0.774133\n",
      "Accuracy on the testing subset: 0.744000\n",
      "\n",
      "Random Forest...\n",
      "Done in 7.785552501678467 seconds...\n",
      "Accuracy on the training subset: 0.815733\n",
      "Accuracy on the testing subset: 0.751200\n",
      "\n",
      "XGBoost Classifier...\n",
      "Done in 298.2690894603729 seconds...\n",
      "Accuracy on the training subset: 0.998933\n",
      "Accuracy on the testing subset: 0.867200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from matplotlib import style\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "clf_A = LogisticRegression(solver='lbfgs',random_state = 42,multi_class='auto')\n",
    "clf_B = SVC(random_state = 912,kernel = 'rbf',gamma = 'scale')\n",
    "clf_C = RandomForestClassifier(n_estimators = 100,max_depth = 10,random_state=82)\n",
    "clf_D = xgb.XGBClassifier(seed=82)\n",
    "\n",
    "print('Logistic Regresssion...')\n",
    "start  = time()\n",
    "clf_A.fit(X_train, y_train)\n",
    "end = time()\n",
    "print('Done in {} seconds...'.format(end - start))\n",
    "y_pred_A = clf_A.predict(X_train)\n",
    "#print(classification_report(y_all,y_pred_A))\n",
    "print('Accuracy on the training subset: {:3f}'.format(clf_A.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(clf_A.score(X_test, y_test)))\n",
    "print('')\n",
    "\n",
    "print('Support Vector Machine...')\n",
    "start  = time()\n",
    "clf_B.fit(X_train, y_train)\n",
    "end = time()\n",
    "print('Done in {} seconds...'.format(end - start))\n",
    "y_pred_B = clf_B.predict(X_train)\n",
    "print('Accuracy on the training subset: {:3f}'.format(clf_B.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(clf_B.score(X_test, y_test)))\n",
    "print('')\n",
    "\n",
    "print('Random Forest...')\n",
    "start = time()\n",
    "clf_C.fit(X_train, y_train)\n",
    "end = time()\n",
    "print('Done in {} seconds...'.format(end - start))\n",
    "y_pred_C = clf_C.predict(X_train)\n",
    "#print(classification_report(y_all,y_pred_C))\n",
    "print('Accuracy on the training subset: {:3f}'.format(clf_C.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(clf_C.score(X_test, y_test)))\n",
    "print('')\n",
    "\n",
    "print('XGBoost Classifier...')\n",
    "start = time()\n",
    "clf_D.fit(X_train, y_train)\n",
    "end = time()\n",
    "print('Done in {} seconds...'.format(end - start))\n",
    "y_pred_D = clf_D.predict(X_train)\n",
    "print('Accuracy on the training subset: {:3f}'.format(clf_D.score(X_train, y_train)))\n",
    "print('Accuracy on the testing subset: {:3f}'.format(clf_D.score(X_test, y_test)))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
