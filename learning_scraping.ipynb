{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate it to srape Asda.com/  Morrisons.com / Sainsburys.com  / Ocado.com\n",
    "\n",
    "# https://youtu.be/XQgXKtPSzUI\n",
    "    \n",
    "# Asp.net page that uses java script to validate user input string \n",
    "# (for validation text with numbers, 6 characters and on space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For chrome and firefox\n",
    "driver = webdriver.Firefox()\n",
    "#driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.amazon.com/s?k='\n",
    "'ultrawide+monitor&crid=267UBIUB9WG7N&sprefix=ultrawide+%2Caps%2C397&ref=nb_sb_ss_ts-a-p_1_10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.amazon.com/s?k='\n",
    "'iphone+12+pro+max&crid=36HYKGS762RC5&sprefix=ipho%2Caps%2C409&ref=nb_sb_noss_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.amazon.com/s?k=ultrawide+monitor&crid=1CJT5GPM3T41H&sprefix=ultra%2Caps%2C395&ref=nb_sb_ss_ts-a-p_1_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.amazon.com/s?k='\n",
    "'playstation+5+console&crid=3QTMXQ2UAA1CO&sprefix=plays%2Caps%2C400&ref=nb_sb_ss_ts-a-p_9_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_term):\n",
    "    \"\"\" Generate a url from search term\"\"\"\n",
    "    template = 'https://www.amazon.com/s?k=' + \\\n",
    "    '{}&crid=267UBIUB9WG7N&sprefix=ultrawide+%2Caps%2C397&ref=nb_sb_ss_ts-a-p_1_10'\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    return template.format(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_url('ultrawide monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exract the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all('div',{'data-component-type': 's-search-result'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atag = item.h2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = atag.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com' + atag.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_parent = item.find('span','a-price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price_parent.find('span','a-offscreen').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = item.i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count = item.find('span',{'class': 'a-size-base','dir': 'auto'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalise the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record(item):\n",
    "    \"\"\" Extract and return data for a single record \"\"\"\n",
    "    # description and url \n",
    "    try:\n",
    "        atag = item.h2.a\n",
    "        description = atag.text.strip()\n",
    "        url = 'https://www.amazon.com' + atag.get('href')\n",
    "    except AttributeError:\n",
    "        description = ''\n",
    "        \n",
    "    # price\n",
    "    try:\n",
    "        price_parent = item.find('span','a-price')\n",
    "        price = price_parent.find('span','a-offscreen').text\n",
    "    except AttributeError:\n",
    "        return\n",
    "    \n",
    "    # rank and rating\n",
    "    try:\n",
    "        rating = item.i.text\n",
    "        review_count = item.find('span',{'class': 'a-size-base','dir': 'auto'}).text\n",
    "    except AttributeError:\n",
    "        rating = ''\n",
    "        review_count = ''\n",
    "    \n",
    "    result = (description, price, rating, review_count, url)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "results = soup.find_all('div',{'data-component-type': 's-search-result'} )\n",
    "\n",
    "for item in results:\n",
    "    record = extract_record(item)\n",
    "    if record:\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in records:\n",
    "    print(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_term):\n",
    "    \"\"\" Generate a url from search term\"\"\"\n",
    "    template = 'https://www.amazon.com/s?k=' + \\\n",
    "    '{}&crid=267UBIUB9WG7N&sprefix=ultrawide+%2Caps%2C397&ref=nb_sb_ss_ts-a-p_1_10'\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    \n",
    "    # add term query to url\n",
    "    url = template.format(search_term)\n",
    "    \n",
    "    # add page query placeholder\n",
    "    url += '&page={}'\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_url('ultrawide monitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url.format(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "\n",
    "def get_url(search_term):\n",
    "    \"\"\" Generate a url from search term\"\"\"\n",
    "    template = 'https://www.amazon.com/s?k=' + \\\n",
    "    '{}&crid=267UBIUB9WG7N&sprefix=ultrawide+%2Caps%2C397&ref=nb_sb_ss_ts-a-p_1_10'\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    \n",
    "    # add term query to url\n",
    "    url = template.format(search_term)\n",
    "    \n",
    "    # add page query placeholder\n",
    "    url += '&page{}'\n",
    "    \n",
    "    return url\n",
    "\n",
    "def extract_record(item):\n",
    "    \"\"\" Extract and return data for a single record \"\"\"\n",
    "    # description and url \n",
    "    try:\n",
    "        atag = item.h2.a\n",
    "        description = atag.text.strip()\n",
    "        url = 'https://www.amazon.com' + atag.get('href')\n",
    "    except AttributeError:\n",
    "        description = ''\n",
    "        \n",
    "    # price\n",
    "    try:\n",
    "        price_parent = item.find('span','a-price')\n",
    "        price = price_parent.find('span','a-offscreen').text\n",
    "    except AttributeError:\n",
    "        return\n",
    "    \n",
    "    # rank and rating\n",
    "    try:\n",
    "        rating = item.i.text\n",
    "        review_count = item.find('span',{'class': 'a-size-base','dir': 'auto'}).text\n",
    "    except AttributeError:\n",
    "        rating = ''\n",
    "        review_count = ''\n",
    "    \n",
    "    result = (description, price, rating, review_count, url)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def main(search_term):\n",
    "    \"\"\" Run main program routine \"\"\"\n",
    "    # For chrome and firefox\n",
    "    driver = webdriver.Firefox()\n",
    "    #driver = webdriver.Chrome()\n",
    "    \n",
    "    records = []\n",
    "    url = get_url(search_term)\n",
    "    \n",
    "    for page in range(1,21):\n",
    "        driver.get(url.format(page))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        results = soup.find_all('div',{'data-component-type': 's-search-result'} )\n",
    "\n",
    "        for item in results:\n",
    "            record = extract_record(item)\n",
    "            if record:\n",
    "                records.append(record)\n",
    "    driver.close()\n",
    "    \n",
    "    # save data to csv file\n",
    "    with open('results.csv','w',newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Description', 'Price', 'Rating', 'Review_count', 'Url'])\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('ultrawide monitor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING MORRISONS.COM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_morrisons = 'https://groceries.morrisons.com/on-offer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For chrome and firefox\n",
    "driver = webdriver.Firefox()\n",
    "#driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    driver.get(url_morrisons)\n",
    "except:\n",
    "    print('Page not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element_by_xpath('//input[@id=\"search\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input.send_keys('butter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exract the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = driver.find_elements_by_xpath('.//li[@class=\"fops-item fops-item--cluster\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[218:221]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = results[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = item.find_element_by_xpath('.//div[@class=\"fop-content\"]//h4').get_attribute(\"title\")#h4.span.text\n",
    "product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantity = item.find_element_by_xpath('.//div[@class=\"fop-content\"]//span[@class=\"fop-catch-weight\"]').text\n",
    "quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = item.find_element_by_xpath('.//div[@class=\"price-group-wrapper\"]//span[@class=\"fop-price\"]').text\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = item.find_element_by_xpath('.//div[@class=\"review-wrapper\"]/span/span[@class=\"fop-rating-inner\"]').get_attribute(\"title\")\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atag = item.find_element_by_xpath('.//div[@class=\"fop-contentWrapper\"]/a').get_attribute(\"href\")\n",
    "atag\n",
    "#('div',{'class','fop-contentWrapper'}).a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url = atag #.attrib['href']\n",
    "product_url "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalise the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_morrisons_record(item):\n",
    "    \"\"\" Extract and return data for a single record \"\"\"\n",
    "    # description and url \n",
    "    try:\n",
    "        price =''\n",
    "        product_name = item.find_element_by_xpath('.//div[@class=\"fop-content\"]//h4').get_attribute(\"title\")\n",
    "        price = item.find_element_by_xpath('.//div[@class=\"price-group-wrapper\"]//span[@class=\"fop-price\"]').text\n",
    "        product_url = item.find_element_by_xpath('.//div[@class=\"fop-contentWrapper\"]/a').get_attribute(\"href\")\n",
    "       \n",
    "    except AttributeError:\n",
    "        product_name = 'NotAvailable'\n",
    "        price = 'NotAvailable'\n",
    "        product_url = 'NotAvailable'\n",
    "    except NoSuchElementException:\n",
    "        product_url = 'wrong tag'\n",
    "    \n",
    "    # price\n",
    "    try:\n",
    "        quantity = item.find_element_by_xpath('.//div[@class=\"fop-content\"]//span[@class=\"fop-catch-weight\"]').text\n",
    "        rating = rating = item.find_element_by_xpath('//div[@class=\"review-wrapper\"]/span/span[@class=\"fop-rating-inner\"]').get_attribute(\"title\")\n",
    "    except AttributeError:\n",
    "        rating = 'NotAvailable'\n",
    "        quantity = 'NotAvailable'\n",
    "    \n",
    "    result = (product_name, quantity, price,product_url, rating)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For chrome and firefox\n",
    "driver = webdriver.Firefox()\n",
    "#driver = webdriver.Chrome()\n",
    "try:\n",
    "    driver.get(url_morrisons)\n",
    "except:\n",
    "    print('Page not found')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element_by_xpath('//input[@id=\"search\"]')\n",
    "search_input.send_keys('onions')\n",
    "search_input.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "records = []\n",
    "results = driver.find_elements_by_xpath('//div[@class=\"fop-contentWrapper\"]')\n",
    "\n",
    "for item in results:\n",
    "    record = extract_morrisons_record(item)\n",
    "    if record:\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in records:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save data to csv file\n",
    "save_to_csv('onions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(filename):\n",
    "    with open(filename + '.csv','w',newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Product Name', 'Quantity','Price', 'Url','Rating'])\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver import Firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create web driver for firefox\n",
    "driver = Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.twitter.com/login')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = driver.find_element_by_xpath('//input[@name=\"session[username_or_email]\"]')\n",
    "username.send_keys('0703960254')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "my_password = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = driver.find_element_by_xpath('//input[@name=\"session[password]\"]')\n",
    "password.send_keys(my_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input_twitter = driver.find_element_by_xpath('//input[@data-testid=\"SearchBox_Search_Input\"]')\n",
    "search_input_twitter.send_keys('#NBSFrontline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input_twitter.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_link_text('Latest').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = cards[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = card.find_element_by_xpath('./div[2]/div[1]//span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = card.find_element_by_xpath('.//time').get_attribute('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replying to \\n@nbstv\\n and \\n@NancyKalembe @NancyKalembe\\n #NBSFrontline #NBSUpdates , to solve the issue of an unleveled playing ground regarding campaign funding, a certain maximum budget by candidates should be enforced by law e.g. 200million. Whoever spends more than stipulated would be disqualified by EC.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# content of the tweet\n",
    "comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "comment + ' ' + responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_count = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_data(card):\n",
    "    \"\"\" Extract data from tweet data \"\"\"\n",
    "    username = card.find_element_by_xpath('./div[2]/div[1]//span').text\n",
    "    handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    # content of the tweet\n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    text = comment + responding\n",
    "    reply_count = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    \n",
    "    tweet = (username, handle, postdate, text, reply_count, retweet_count, like_count)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beacon',\n",
       " '@BeaconUg',\n",
       " '2020-12-10T21:14:12.000Z',\n",
       " 'Replying to \\n@nbstv\\n and \\n@NancyKalembe@NancyKalembe\\n #NBSFrontline #NBSUpdates , to solve the issue of an unleveled playing ground regarding campaign funding, a certain maximum budget by candidates should be enforced by law e.g. 200million. Whoever spends more than stipulated would be disqualified by EC.',\n",
       " '',\n",
       " '',\n",
       " '')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tweet_data(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "for card in cards:\n",
    "    data = get_tweet_data(card)\n",
    "    if data:\n",
    "        tweet_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I Hate UMEME',\n",
       " '@Umwamikazi11',\n",
       " '2020-12-10T21:03:29.000Z',\n",
       " 'As If Ssegona has been on a media tour today. Saw him on BBS terefayina in the morning. Now this #nbsfrontline',\n",
       " '2',\n",
       " '',\n",
       " '')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n",
      "Stale element for username\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import Firefox\n",
    "\n",
    "def get_tweet_data(card):\n",
    "    \"\"\" Extract data from tweet data \"\"\"\n",
    "#     ignored_exceptions=(StaleElementReferenceException,)\n",
    "#     sleep(5)\n",
    "    try:\n",
    "        \n",
    "        username = card.find_element_by_xpath('.//span').text\n",
    "    #WebDriverWait(driver, 20,ignored_exceptions=ignored_exceptions).until(EC.element_to_be_clickable((By.XPATH, \\\n",
    "    #'./div[2]/div[1]//span')))\n",
    "    except StaleElementReferenceException:\n",
    "        print('Stale element for username')\n",
    "        username = ''\n",
    "        \n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except StaleElementReferenceException:\n",
    "        handle = ''\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    except StaleElementReferenceException:\n",
    "        postdate = ''\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # content of the tweet\n",
    "        comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "        responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    \n",
    "        text = comment + responding\n",
    "    except StaleElementReferenceException:\n",
    "        text = ''\n",
    "    try:\n",
    "        reply_count = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    except StaleElementReferenceException:\n",
    "        reply_count = ''\n",
    "        \n",
    "    try:\n",
    "        retweet_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    except StaleElementReferenceException:\n",
    "        retweet_count = ''\n",
    "    try:\n",
    "        like_count = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    except StaleElementReferenceException:\n",
    "        like_count = ''\n",
    "        \n",
    "    \n",
    "    tweet = (username, handle, postdate, text, reply_count, retweet_count, like_count)\n",
    "    return tweet\n",
    "\n",
    "# create web driver for firefox\n",
    "driver = Firefox()\n",
    "driver.get('https://www.twitter.com/login')\n",
    "driver.maximize_window()\n",
    "\n",
    "username = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \\\n",
    "    '//input[@name=\"session[username_or_email]\"]')))\n",
    "        \n",
    "\n",
    "#username = driver.find_element_by_xpath('//input[@name=\"session[username_or_email]\"]')\n",
    "username.send_keys('0703960254')\n",
    "my_password = getpass()\n",
    "\n",
    "password = driver.find_element_by_xpath('//input[@name=\"session[password]\"]')\n",
    "password.send_keys(my_password)\n",
    "password.send_keys(Keys.RETURN)\n",
    "sleep(1)\n",
    "\n",
    "search_input_twitter = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \\\n",
    "    '//input[@data-testid=\"SearchBox_Search_Input\"]')))\n",
    "#search_input_twitter = driver.find_element_by_xpath('//input[@data-testid=\"SearchBox_Search_Input\"]')\n",
    "search_input_twitter.send_keys('#venturecapital')\n",
    "search_input_twitter.send_keys(Keys.RETURN)\n",
    "\n",
    "# navigate to the latest tab\n",
    "# try:\n",
    "element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.LINK_TEXT, \"Latest\"))) \n",
    "    #'//input[@data-testid=\"SearchBox_Search_Input\"]')))\n",
    "element.click()\n",
    "# except:\n",
    "#     print('Could not access the \"Latest\" tab. Perhaps you are using a different language')\n",
    "    \n",
    "ignored_exceptions=(StaleElementReferenceException,)\n",
    "    \n",
    "# get all tweets on the page\n",
    "data = []\n",
    "tweet_ids = set()\n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "scrolling = True\n",
    "\n",
    "while scrolling:\n",
    "    page_cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')\n",
    "    for card in page_cards[-15:]:\n",
    "        \n",
    "        tweet = get_tweet_data(card)\n",
    "        if tweet:\n",
    "            tweet_id = ''.join(tweet)\n",
    "            if tweet_id not in tweet_ids:\n",
    "                tweet_ids.add(tweet_id)\n",
    "                data.append(tweet)\n",
    "    \n",
    "    scroll_attempt = 0\n",
    "    while True:\n",
    "        # check scroll position\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1)\n",
    "        current_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == current_position:\n",
    "            scroll_attempt += 1\n",
    "            \n",
    "            # end of scroll region\n",
    "            if scroll_attempt >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else:\n",
    "                sleep(2) # attempt to scroll again\n",
    "        else:\n",
    "            last_position = current_position\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WWW.WIX.VC Venture Capital',\n",
       " '@Nauticus6',\n",
       " '2020-12-09T08:47:11.000Z',\n",
       " 'Replying to \\n@RampCapitalLLCEasy you buy a premium VC domain name and wait.  #vc #VentureCapital #virtualcompany #virtualclass #virtualcurrency #virtualcoin #domainsforsale',\n",
       " '',\n",
       " '',\n",
       " '')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_twitter_to_csv(filename):\n",
    "    with open(filename + '.csv','w',newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Username', 'Handle','Postdate', 'Text','Reply Count','Retweet Count','Like Count'])\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_twitter_to_csv('venturecapital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_Links = WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@id='pages-2']/div/ul/li/a[@href]\")))\n",
    "\n",
    "# for link in all_Links:\n",
    "#     print link.get_attribute(\"href\")\n",
    "#     links.append(link.get_attribute(\"href\"))\n",
    "\n",
    "# for link in links:\n",
    "#     driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = driver.find_elements_by_xpath('//*[@id=\"pages-2\"]/div/ul/li/a')\n",
    "# links_hrefs = [link.get_attribute('href') for link in links]\n",
    "\n",
    "# for i in links_hrefs:\n",
    "#     driver.get(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
